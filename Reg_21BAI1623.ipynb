{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Philip** **George** **Varughese**\n",
        "# **21BAI1623**\n",
        "\n",
        "#Naive bayes Classifier:\n",
        "\n",
        "Consider two datasets that are appropriate to implement your allocated machine learning (ML) model or you can create your own datasets as well.\n",
        "\n",
        "Then compute the accuracy for both datasets with train+test split accuracy as:\n",
        "*   50+50\n",
        "*   60+40\n",
        "*   70+30\n",
        "*   80+20\n",
        "*   90+10\n",
        "\n",
        "#In the Report:\n",
        "\n",
        "Justification for choice of datasets\n",
        "\n",
        "Dataset References link\n",
        "\n",
        "\n",
        "The class prior probabilities form the training set\n",
        "\n",
        "Accuracy table for all split ratios for both datasets\n",
        "\n",
        "Identify which split is performing better and provide justification\n",
        "\n",
        "#Note:\n",
        "\n",
        "Upload Report and .ipynb file as single Zip file named as \"Reg.No_Name\"\n"
      ],
      "metadata": {
        "id": "Ls8n916AnLWQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the Iris.csv Dataset:"
      ],
      "metadata": {
        "id": "cRhkbHwdu6VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader\n",
        "from random import seed, randrange\n",
        "from math import sqrt, exp, pi\n",
        "\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        if row[column].strip() == \"\":\n",
        "            row[column] = 0.0\n",
        "        else:\n",
        "            row[column] = float(row[column].strip())\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "def separate_by_class(dataset):\n",
        "    separated = dict()\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        class_value = vector[-1]\n",
        "        if class_value not in separated:\n",
        "            separated[class_value] = list()\n",
        "        separated[class_value].append(vector)\n",
        "    return separated\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([(x - avg) ** 2 for x in numbers]) / float(len(numbers) - 1)\n",
        "    return sqrt(variance)\n",
        "def summarize_dataset(dataset):\n",
        "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "def summarize_by_class(dataset):\n",
        "    separated = separate_by_class(dataset)\n",
        "    summaries = dict()\n",
        "    for class_value, rows in separated.items():\n",
        "        summaries[class_value] = summarize_dataset(rows)\n",
        "    return summaries\n",
        "def calculate_probability(x, mean, stdev):\n",
        "    exponent = exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
        "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
        "def calculate_class_probabilities(summaries, row):\n",
        "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "    probabilities = dict()\n",
        "    for class_value, class_summaries in summaries.items():\n",
        "        probabilities[class_value] = summaries[class_value][0][2] / float(total_rows)\n",
        "        for i in range(len(class_summaries)):\n",
        "            mean, stdev, _ = class_summaries[i]\n",
        "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
        "    return probabilities\n",
        "def predict(summaries, row):\n",
        "    probabilities = calculate_class_probabilities(summaries, row)\n",
        "    best_label, best_prob = None, -1\n",
        "    for class_value, probability in probabilities.items():\n",
        "        if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = class_value\n",
        "    return best_label\n",
        "def naive_bayes(train, test):\n",
        "    summarize = summarize_by_class(train)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        output = predict(summarize, row)\n",
        "        predictions.append(output)\n",
        "    return predictions\n",
        "seed(1)\n",
        "filename = '/content/Iris.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0]) - 1):\n",
        "    str_column_to_float(dataset, i)\n",
        "class_column_index = len(dataset[0]) - 1\n",
        "str_column_to_int(dataset, class_column_index)\n",
        "n_folds = 50\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"50-50:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 40\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"60-40:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 30\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"70-30:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 20\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"80-20:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 10\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"90-10:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNg2TyVbhaxI",
        "outputId": "039306a2-14a7-45cc-fe81-ac71d67a7666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50-50:\n",
            "Scores: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 66.66666666666666, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "Mean Accuracy: 99.333%\n",
            "\n",
            "\n",
            "60-40:\n",
            "Scores: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 66.66666666666666, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "Mean Accuracy: 99.167%\n",
            "\n",
            "\n",
            "70-30:\n",
            "Scores: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 80.0]\n",
            "Mean Accuracy: 99.333%\n",
            "\n",
            "\n",
            "80-20:\n",
            "Scores: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 85.71428571428571, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
            "Mean Accuracy: 99.286%\n",
            "\n",
            "\n",
            "90-10:\n",
            "Scores: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 93.33333333333333]\n",
            "Mean Accuracy: 99.333%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the adult-health.csv Dataset:"
      ],
      "metadata": {
        "id": "FSJ6kyrhoiEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from csv import reader\n",
        "from random import seed, randrange\n",
        "from math import sqrt, exp, pi\n",
        "\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        next(csv_reader)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "def str_column_to_float(dataset, column):\n",
        "    for row in dataset:\n",
        "        if row[column].strip() == \"\":\n",
        "            row[column] = 0.0\n",
        "        else:\n",
        "            row[column] = float(row[column].strip())\n",
        "def str_column_to_int(dataset, column):\n",
        "    class_values = [row[column] for row in dataset]\n",
        "    unique = set(class_values)\n",
        "    lookup = dict()\n",
        "    for i, value in enumerate(unique):\n",
        "        lookup[value] = i\n",
        "    for row in dataset:\n",
        "        row[column] = lookup[row[column]]\n",
        "    return lookup\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "    dataset_split = list()\n",
        "    dataset_copy = list(dataset)\n",
        "    fold_size = int(len(dataset) / n_folds)\n",
        "    for _ in range(n_folds):\n",
        "        fold = list()\n",
        "        while len(fold) < fold_size:\n",
        "            index = randrange(len(dataset_copy))\n",
        "            fold.append(dataset_copy.pop(index))\n",
        "        dataset_split.append(fold)\n",
        "    return dataset_split\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "    folds = cross_validation_split(dataset, n_folds)\n",
        "    scores = list()\n",
        "    for fold in folds:\n",
        "        train_set = list(folds)\n",
        "        train_set.remove(fold)\n",
        "        train_set = sum(train_set, [])\n",
        "        test_set = list()\n",
        "        for row in fold:\n",
        "            row_copy = list(row)\n",
        "            test_set.append(row_copy)\n",
        "            row_copy[-1] = None\n",
        "        predicted = algorithm(train_set, test_set, *args)\n",
        "        actual = [row[-1] for row in fold]\n",
        "        accuracy = accuracy_metric(actual, predicted)\n",
        "        scores.append(accuracy)\n",
        "    return scores\n",
        "def separate_by_class(dataset):\n",
        "    separated = dict()\n",
        "    for i in range(len(dataset)):\n",
        "        vector = dataset[i]\n",
        "        class_value = vector[-1]\n",
        "        if class_value not in separated:\n",
        "            separated[class_value] = list()\n",
        "        separated[class_value].append(vector)\n",
        "    return separated\n",
        "def mean(numbers):\n",
        "    return sum(numbers) / float(len(numbers))\n",
        "def stdev(numbers):\n",
        "    avg = mean(numbers)\n",
        "    variance = sum([(x - avg) ** 2 for x in numbers]) / float(len(numbers) - 1)\n",
        "    return sqrt(variance)\n",
        "def summarize_dataset(dataset):\n",
        "    summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "    del summaries[-1]\n",
        "    return summaries\n",
        "def summarize_by_class(dataset):\n",
        "    separated = separate_by_class(dataset)\n",
        "    summaries = dict()\n",
        "    for class_value, rows in separated.items():\n",
        "        summaries[class_value] = summarize_dataset(rows)\n",
        "    return summaries\n",
        "def calculate_probability(x, mean, stdev):\n",
        "    exponent = exp(-((x - mean) ** 2 / (2 * stdev ** 2)))\n",
        "    return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
        "def calculate_class_probabilities(summaries, row):\n",
        "    total_rows = sum([summaries[label][0][2] for label in summaries])\n",
        "    probabilities = dict()\n",
        "    for class_value, class_summaries in summaries.items():\n",
        "        probabilities[class_value] = summaries[class_value][0][2] / float(total_rows)\n",
        "        for i in range(len(class_summaries)):\n",
        "            mean, stdev, _ = class_summaries[i]\n",
        "            probabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
        "    return probabilities\n",
        "def predict(summaries, row):\n",
        "    probabilities = calculate_class_probabilities(summaries, row)\n",
        "    best_label, best_prob = None, -1\n",
        "    for class_value, probability in probabilities.items():\n",
        "        if best_label is None or probability > best_prob:\n",
        "            best_prob = probability\n",
        "            best_label = class_value\n",
        "    return best_label\n",
        "def naive_bayes(train, test):\n",
        "    summarize = summarize_by_class(train)\n",
        "    predictions = list()\n",
        "    for row in test:\n",
        "        output = predict(summarize, row)\n",
        "        predictions.append(output)\n",
        "    return predictions\n",
        "seed(1)\n",
        "filename = '/content/adult-health.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0]) - 1):\n",
        "    str_column_to_float(dataset, i)\n",
        "class_column_index = len(dataset[0]) - 1\n",
        "str_column_to_int(dataset, class_column_index)\n",
        "n_folds = 50\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"50-50:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 40\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"60-40:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 30\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"70-30:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 20\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"80-20:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))\n",
        "print(\"\\n\")\n",
        "n_folds = 10\n",
        "scores = evaluate_algorithm(dataset, naive_bayes, n_folds)\n",
        "print(\"90-10:\")\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores) / float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFiV6J7Io2NP",
        "outputId": "7446dd9d-7e56-4f2f-a59f-95aa88e4b5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50-50:\n",
            "Scores: [100.0, 94.73684210526315, 84.21052631578947, 94.73684210526315, 94.73684210526315, 89.47368421052632, 84.21052631578947, 94.73684210526315, 89.47368421052632, 89.47368421052632, 78.94736842105263, 84.21052631578947, 94.73684210526315, 94.73684210526315, 94.73684210526315, 94.73684210526315, 89.47368421052632, 94.73684210526315, 100.0, 94.73684210526315, 100.0, 100.0, 89.47368421052632, 89.47368421052632, 94.73684210526315, 89.47368421052632, 100.0, 94.73684210526315, 94.73684210526315, 84.21052631578947, 100.0, 94.73684210526315, 84.21052631578947, 100.0, 100.0, 94.73684210526315, 100.0, 94.73684210526315, 94.73684210526315, 89.47368421052632, 84.21052631578947, 89.47368421052632, 100.0, 94.73684210526315, 100.0, 84.21052631578947, 94.73684210526315, 100.0, 94.73684210526315, 94.73684210526315]\n",
            "Mean Accuracy: 93.263%\n",
            "\n",
            "\n",
            "60-40:\n",
            "Scores: [91.66666666666666, 91.66666666666666, 87.5, 95.83333333333334, 100.0, 95.83333333333334, 100.0, 100.0, 83.33333333333334, 100.0, 91.66666666666666, 87.5, 95.83333333333334, 87.5, 91.66666666666666, 91.66666666666666, 91.66666666666666, 91.66666666666666, 95.83333333333334, 87.5, 100.0, 91.66666666666666, 95.83333333333334, 91.66666666666666, 100.0, 95.83333333333334, 100.0, 87.5, 95.83333333333334, 95.83333333333334, 91.66666666666666, 91.66666666666666, 91.66666666666666, 100.0, 100.0, 87.5, 91.66666666666666, 100.0, 83.33333333333334, 87.5]\n",
            "Mean Accuracy: 93.438%\n",
            "\n",
            "\n",
            "70-30:\n",
            "Scores: [90.9090909090909, 93.93939393939394, 84.84848484848484, 96.96969696969697, 87.87878787878788, 81.81818181818183, 90.9090909090909, 100.0, 96.96969696969697, 96.96969696969697, 96.96969696969697, 90.9090909090909, 87.87878787878788, 93.93939393939394, 96.96969696969697, 100.0, 93.93939393939394, 96.96969696969697, 96.96969696969697, 84.84848484848484, 90.9090909090909, 96.96969696969697, 96.96969696969697, 96.96969696969697, 90.9090909090909, 96.96969696969697, 93.93939393939394, 90.9090909090909, 93.93939393939394, 84.84848484848484]\n",
            "Mean Accuracy: 93.131%\n",
            "\n",
            "\n",
            "80-20:\n",
            "Scores: [91.83673469387756, 91.83673469387756, 93.87755102040816, 89.79591836734694, 97.95918367346938, 93.87755102040816, 93.87755102040816, 91.83673469387756, 93.87755102040816, 87.75510204081633, 100.0, 89.79591836734694, 89.79591836734694, 93.87755102040816, 95.91836734693877, 89.79591836734694, 97.95918367346938, 89.79591836734694, 93.87755102040816, 97.95918367346938]\n",
            "Mean Accuracy: 93.265%\n",
            "\n",
            "\n",
            "90-10:\n",
            "Scores: [94.94949494949495, 92.92929292929293, 93.93939393939394, 89.8989898989899, 93.93939393939394, 93.93939393939394, 91.91919191919192, 96.96969696969697, 90.9090909090909, 93.93939393939394]\n",
            "Mean Accuracy: 93.333%\n"
          ]
        }
      ]
    }
  ]
}